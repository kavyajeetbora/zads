{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/zads/blob/master/development/02_Get_POIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet duckdb\n",
        "!pip install --quiet jupysql\n",
        "!pip install --quiet duckdb-engine\n",
        "!pip install -q pydeck\n",
        "!touch __init__.py"
      ],
      "metadata": {
        "id": "OBAe4YKl86Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OOdwVBZ8ads"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import pydeck as pdk\n",
        "import shapely\n",
        "import duckdb\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Import jupysql Jupyter extension to create SQL cells\n",
        "%load_ext sql\n",
        "\n",
        "from lxml import etree\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import math\n",
        "import re\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "13YHc6YxB9TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%config SqlMagic.autopandas = True\n",
        "%config SqlMagic.feedback = False\n",
        "%config SqlMagic.displaycon = False\n",
        "\n",
        "%sql duckdb:///:memory:\n",
        "# %sql duckdb:///path/to/file.db"
      ],
      "metadata": {
        "id": "qWoVUmQK-wCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "INSTALL httpfs;\n",
        "INSTALL spatial;"
      ],
      "metadata": {
        "id": "xT0q4jocBqSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bounding Box:"
      ],
      "metadata": {
        "id": "Nn4IZujxBxsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W,S,E,N = 91.7730008646,26.1579902082,91.7868625208,26.1682553455\n",
        "bbox = shapely.box(W,S,E,N)\n",
        "lon,lat = [x[0] for x in bbox.centroid.xy]\n",
        "print(lon,lat)"
      ],
      "metadata": {
        "id": "FRDrTlhXBmNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Oveture map latest release"
      ],
      "metadata": {
        "id": "p2la5kjWBzO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    url = r'https://docs.overturemaps.org/release/latest/'\n",
        "    req = requests.get(url)\n",
        "    html_text = req.text\n",
        "    tree = etree.HTML(html_text)\n",
        "\n",
        "    result = tree.xpath('//h1')\n",
        "    latest_release = result[0].text\n",
        "    print(\"latest Overture map release:\" ,latest_release)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error\", e)"
      ],
      "metadata": {
        "id": "mQ8zQYZLB3bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Building Footprints\n",
        "\n",
        "[Refer place schema references](https://docs.overturemaps.org/schema/reference/places/place/)"
      ],
      "metadata": {
        "id": "eSdNrB_CCCpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Check if the file already exists and remove it\n",
        "if os.path.exists('overture_places.geojson'):\n",
        "    os.remove('overture_places.geojson')\n",
        "\n",
        "places_data_url = rf\"s3://overturemaps-us-west-2/release/{latest_release}/theme=places/type=*/*\"\n",
        "\n",
        "con = duckdb.connect()\n",
        "con.execute(\"INSTALL spatial\") # Install the spatial extension, which includes GDAL\n",
        "con.execute(\"LOAD spatial\")  # Load the spatial extension\n",
        "\n",
        "df = con.sql(\n",
        "    f'''\n",
        "    COPY(\n",
        "        SELECT\n",
        "            pois.names.primary as name,\n",
        "            pois.categories.primary as category,\n",
        "            pois.confidence as confindence,\n",
        "            pois.websites[1] as website,\n",
        "            pois.addresses[1].postcode as Address,\n",
        "            pois.phones[1] as phone_number,\n",
        "            pois.geometry\n",
        "        FROM read_parquet('{places_data_url}', filename=true, hive_partitioning=1) AS pois\n",
        "        WHERE pois.bbox.xmin > {W}\n",
        "        AND pois.bbox.xmax < {E}\n",
        "        AND pois.bbox.ymin > {S}\n",
        "        AND pois.bbox.ymax < {N}\n",
        "    ) TO 'overture_places.geojson' WITH (FORMAT GDAL, DRIVER 'GeoJSON');\n",
        "    '''\n",
        ")\n",
        "con.close() # Close the connection"
      ],
      "metadata": {
        "id": "wZwLPxItB_Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize the data"
      ],
      "metadata": {
        "id": "BZjO6dHmF2PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdf = gpd.read_file(\"overture_places.geojson\")\n",
        "gdf['coordinates'] = list(zip(gdf['geometry'].x, gdf['geometry'].y))\n",
        "gdf = gdf.drop('geometry',axis=1)\n",
        "print(f\"Total number of point of interests found: {gdf.shape[0]}\")\n",
        "gdf.head()"
      ],
      "metadata": {
        "id": "NLA-XereFowt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf['category'].value_counts()"
      ],
      "metadata": {
        "id": "4C6oM264q2Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several NLP techniques and tools that can help you categorize your data intelligently. Here are a few approaches you can consider:\n",
        "\n",
        "1. Manual Mapping with NLP Assistance:\n",
        "Use pre-trained models like BERT or GPT to understand the context of each category and suggest appropriate bins.\n",
        "Tools like spaCy or NLTK can help you preprocess and analyze the text data to find similarities and differences between categories.\n",
        "2. Clustering Algorithms:\n",
        "Algorithms like K-means or hierarchical clustering can group similar categories together based on their features.\n",
        "You can use libraries like Scikit-learn in Python to implement these clustering techniques.\n",
        "3. Topic Modeling:\n",
        "Techniques like Latent Dirichlet Allocation (LDA) can help identify topics within your categories and group them accordingly.\n",
        "Libraries like Gensim can be used for topic modeling.\n",
        "4. Custom Classification Models:\n",
        "Train a custom classifier using labeled data to predict the bin for each category.\n",
        "Use machine learning frameworks like TensorFlow or PyTorch to build and train your model.\n",
        "5. Pre-built Tools and APIs:\n",
        "Tools like IBM Watson, Google Cloud Natural Language, or Microsoft Azure Text Analytics offer APIs that can categorize text data based on predefined or custom categories."
      ],
      "metadata": {
        "id": "C7m8HrTQ9FnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading data from google maps API\n",
        "\n",
        "Some refereneces\n",
        "\n",
        "1. https://serpapi.com/blog/how-we-reverse-engineered-google-maps-pagination/"
      ],
      "metadata": {
        "id": "jXfJ0YuP-LZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PAGINATION_PARAMETERS_REGEX = re.compile(\n",
        "    r\"\"\"\n",
        "    \\A                                      # Start of string\n",
        "    (?:\\s*)                                 # Initial possible whitespace\n",
        "    @(?P<latitude>[-+]?\\d{1,2}(?:[.,]\\d+)?)  # Latitude: @10.78472\n",
        "    (?:\\s*,\\s*)                             # Separator between latitude and longitude\n",
        "    (?P<longitude>[-+]?\\d{1,3}(?:[.,]\\d+)?)  # Longitude: @-110\n",
        "    (?:\\s*,\\s*)                             # Separator between longitude and zoom\n",
        "    (?P<zoom>\\d{1,2}(?:[.,]\\d+)?)z           # Zoom: 9.22\n",
        "    $                                      # End of string\n",
        "    \"\"\", re.VERBOSE\n",
        "\n",
        ")\n",
        "\n",
        "EARTH_RADIUS_IN_METERS = 6371010\n",
        "TILE_SIZE = 256\n",
        "SCREEN_PIXEL_HEIGHT = 768\n",
        "RADIUS_X_PIXEL_HEIGHT = 27.3611 * EARTH_RADIUS_IN_METERS * SCREEN_PIXEL_HEIGHT\n",
        "\n",
        "def altitude(zoom, latitude):\n",
        "    return str((RADIUS_X_PIXEL_HEIGHT * math.cos((latitude * math.pi) / 180)) / ((2 ** zoom) * TILE_SIZE))\n",
        "\n",
        "def pagination(location_lat_long, start_offset):\n",
        "    extracted_parameters = PAGINATION_PARAMETERS_REGEX.match(location_lat_long)\n",
        "\n",
        "    if not extracted_parameters:\n",
        "        return \"\"\n",
        "\n",
        "    return (\n",
        "        \"!4m8!1m3!1d\" +\n",
        "        altitude(float(extracted_parameters['zoom']), float(extracted_parameters['latitude'])) +\n",
        "        \"!2d\" +\n",
        "        extracted_parameters['longitude'] +\n",
        "        \"!3d\" +\n",
        "        extracted_parameters['latitude'] +\n",
        "        \"!3m2!1i1024!2i768!4f13.1!7i20!8i\" +\n",
        "        (start_offset or \"0\") +\n",
        "        \"!10b1!12m25!1m1!18b1!2m3!5m1!6e2!20e3!6m16!4b1!23b1!26i1!27i1!41i2!45b1!49b1!63m0!67b1!73m0!74i150000!75b1!89b1!105b1!109b1!110m0!10b1!16b1!19m4!2m3!1i360!2i120!4i8!20m65!2m2!1i203!2i100!3m2!2i4!5b1!6m6!1m2!1i86!2i86!1m2!1i408!2i240!7m50!1m3!1e1!2b0!3e3!1m3!1e2!2b1!3e2!1m3!1e2!2b0!3e3!1m3!1e3!2b0!3e3!1m3!1e8!2b0!3e3!1m3!1e3!2b1!3e2!1m3!1e10!2b0!3e3!1m3!1e10!2b1!3e2!1m3!1e9!2b1!3e2!1m3!1e10!2b0!3e3!1m3!1e10!2b1!3e2!1m3!1e10!2b0!3e4!2b1!4b1!9b0!22m3!1s!2z!7e81!24m55!1m15!13m7!2b1!3b1!4b1!6i1!8b1!9b1!20b0!18m6!3b1!4b1!5b1!6b1!13b0!14b0!2b1!5m5!2b1!3b1!5b1!6b1!7b1!10m1!8e3!14m1!3b1!17b1!20m4!1e3!1e6!1e14!1e15!24b1!25b1!26b1!29b1!30m1!2b1!36b1!43b1!52b1!54m1!1b1!55b1!56m2!1b1!3b1!65m5!3m4!1m3!1m2!1i224!2i298!89b1!26m4!2m3!1i80!2i92!4i8!30m28!1m6!1m2!1i0!2i0!2m2!1i458!2i768!1m6!1m2!1i974!2i0!2m2!1i1024!2i768!1m6!1m2!1i0!2i0!2m2!1i1024!2i20!1m6!1m2!1i0!2i748!2m2!1i1024!2i768!34m16!2b1!3b1!4b1!6b1!8m4!1b1!3b1!4b1!6b1!9b1!12b1!14b1!20b1!23b1!25b1!26b1!37m1!1e81!42b1!46m1!1e9!47m0!49m1!3b1!50m53!1m49!2m7!1u3!4s!5e1!9s!10m2!3m1!1e1!2m7!1u2!4s!5e1!9s!10m2!2m1!1e1!2m7!1u16!4s!5e1!9s!10m2!16m1!1e1!2m7!1u16!4s!5e1!9s!10m2!16m1!1e2!3m11!1u16!2m4!1m2!16m1!1e1!2s!2m4!1m2!16m1!1e2!2s!3m1!1u2!3m1!1u3!4BIAE!2e2!3m1!3b1!59B!65m0!69i540\"\n",
        "    )\n",
        "\n",
        "def place_url(url):\n",
        "  pattern = r\"placeid=([^\\&]+)\"\n",
        "  match = re.search(pattern, url)\n",
        "  return 'https://www.google.com/maps/place/?q=place_id:' + match.group(1)\n",
        "\n",
        "def pagination_to_pandas(messy_data):\n",
        "    data = json.loads(messy_data[:-6])\n",
        "    javascript_array_str = data[\"d\"][5:]\n",
        "    python_list = json.loads(javascript_array_str)\n",
        "    places_data = python_list[0][1]\n",
        "\n",
        "    name = [i[14][11] for i in places_data[1:]]\n",
        "    Addresses = [i[14][2] for i in places_data[1:]]\n",
        "    website = [i[14][7] for i in places_data[1:]]\n",
        "    phone_number = [i[14][178] for i in places_data[1:]]\n",
        "    open_close_timing = [i[14][34] for i in places_data[1:]]\n",
        "    reviews_rating = [i[14][4] for i in places_data[1:]]\n",
        "\n",
        "\n",
        "    places_dict = {\n",
        "        \"name\": [i for i in name],\n",
        "        \"address\": [i[0] for i in Addresses],\n",
        "        \"website\": [i[0] if i is not None else \"None\" for i in website],\n",
        "        \"phone_number\": [i for i in phone_number],\n",
        "        \"open_close_timing\": [i for i in open_close_timing],\n",
        "        \"reviews_rating\": [i for i in reviews_rating],\n",
        "        \"latitude\": [\n",
        "            i[14][9][2] if i[14][9] is not None else None for i in places_data[1:]\n",
        "        ],\n",
        "        \"longitude\": [\n",
        "            i[14][9][3] if i[14][9] is not None else None for i in places_data[1:]\n",
        "        ],\n",
        "    }\n",
        "    places_dict[\"phone_number\"] = [\n",
        "        data[0][0] if data is not None and data[0] is not None else \"None\"\n",
        "        for data in places_dict[\"phone_number\"]\n",
        "    ]\n",
        "\n",
        "    timings = []\n",
        "    for list_index in range(len(places_dict[\"open_close_timing\"])):\n",
        "        temp_dic = {}\n",
        "        if places_dict[\"open_close_timing\"][list_index]:\n",
        "            if places_dict[\"open_close_timing\"][list_index][1]:\n",
        "                for i in range(7):\n",
        "                    temp_dic[places_dict[\"open_close_timing\"][list_index][1][i][0]] = (\n",
        "                        places_dict[\"open_close_timing\"][list_index][1][i][1][0].replace(\n",
        "                            \"\\u202f\", \" \"\n",
        "                        )\n",
        "                    )\n",
        "        else:\n",
        "            temp_dic[\"days\"] = None\n",
        "        timings.append(temp_dic)\n",
        "    places_dict[\"open_close_timing\"] = [i for i in timings]\n",
        "\n",
        "    places_dict[\"ratings\"] = [\n",
        "        i[7] if i is not None else None for i in places_dict[\"reviews_rating\"]\n",
        "    ]\n",
        "    places_dict[\"reviews\"] = [\n",
        "        i[3][1] if i is not None else None for i in places_dict[\"reviews_rating\"]\n",
        "    ]\n",
        "    places_dict[\"gmap_link\"] = [\n",
        "        place_url(x) if x is not None else None\n",
        "        for x in [i[3][0] if i is not None else None for i in places_dict[\"reviews_rating\"]]\n",
        "    ]\n",
        "    if \"reviews_rating\" in places_dict.keys():\n",
        "        del places_dict[\"reviews_rating\"]\n",
        "    else:\n",
        "        print('It\"s Already deleted')\n",
        "\n",
        "    ## Export to pandas dataframe\n",
        "    df = pd.DataFrame(places_dict)\n",
        "    new_order = [\n",
        "        \"name\",\n",
        "        \"latitude\",\n",
        "        \"longitude\",\n",
        "        \"phone_number\",\n",
        "        \"ratings\",\n",
        "        \"reviews\",\n",
        "        \"website\",\n",
        "        \"address\",\n",
        "        \"gmap_link\",\n",
        "        \"open_close_timing\"\n",
        "    ]\n",
        "    df = df[new_order]\n",
        "    df[\"address\"] = df.address.astype(str).str.replace(\"[\", \"\").str.replace(\"]\", \"\")\n",
        "    df = df.astype(object)\n",
        "    df.ratings = df.ratings.astype(str)\n",
        "    df.open_close_timing = df.open_close_timing.astype(str)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "fdDJkrKPvqMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the messy data to dataframe"
      ],
      "metadata": {
        "id": "VY7sW_bwAPkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "place_name = 'restaurants'\n",
        "\n",
        "zoom_level = 16\n",
        "location_lat_long = f\"@{lat},{lon},{zoom_level}z\"\n",
        "\n",
        "headers = {\n",
        "      'User-Agent': 'Mozilla/5.0 (X11; CrOS x86_64 14541.0.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "      'Cookie': \"place_your_cookie_here\",\n",
        "      'Referer': 'https://www.google.com/',\n",
        "      'Accept' : '*/*',\n",
        "      'Accept-Language' : 'en-US,en;q=0.9'\n",
        "}\n",
        "\n",
        "url = \"https://www.google.com/search\"\n",
        "\n",
        "results = []\n",
        "\n",
        "for i in tqdm(range(20,200,20)):\n",
        "    start_offset = f'{i}'\n",
        "\n",
        "    params = {\n",
        "        'tbm': 'map',\n",
        "        'authuser': '0',\n",
        "        'hl': 'en',\n",
        "        'pb': pagination(location_lat_long, start_offset),  # Replace 'your_partial_pb_value' with the actual value for the 'pb' key\n",
        "        'q': place_name,\n",
        "        'tch': '1',\n",
        "        'ech': '5',\n",
        "        \"near\": f\"{lat},{lon}\",  # This is the key parameter for location binding\n",
        "    }\n",
        "    response = requests.get(url, params=params, headers=headers)\n",
        "    messy_data = response.text\n",
        "\n",
        "    df = pagination_to_pandas(messy_data)\n",
        "    if df.shape[0]>0:\n",
        "        results.append(df)\n",
        "    else:\n",
        "        print(f\"No more data found at offset value: {i}\")\n",
        "        break\n",
        "\n",
        "results = pd.concat(results)\n",
        "results = results.drop_duplicates(subset=['gmap_link', 'name'])\n",
        "results['coordinates'] = list(zip(results['longitude'], results['latitude']))\n",
        "results.drop(['latitude', 'longitude'],axis=1, inplace=True)\n",
        "print(\"Total Unique Restaurants found:\",results.shape[0])\n",
        "\n",
        "results.sample(10)"
      ],
      "metadata": {
        "id": "dZxIYGC2BU-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge Data\n",
        "\n",
        "once the data from both sources are fetched, concat the data into one for visualization"
      ],
      "metadata": {
        "id": "SrR8uhmWgnEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = pd.concat([results, gdf])\n",
        "print(\"Total nearby restaurants found:\",df_final.shape[0])\n",
        "df_final.sample(6)"
      ],
      "metadata": {
        "id": "nCgKQ2I2gwRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the data"
      ],
      "metadata": {
        "id": "cuPTZFbsEsWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tooltip = {\n",
        "  \"text\": \"ðŸ·ï¸ Name: {name}\\nðŸ“ž Phone Number: {phone_number}\\nâ­ Ratings: {ratings}\\nðŸ“ Reviews: {reviews}\\nðŸŒ Website: {website}\\nðŸ“ Address: {address}\"\n",
        "}"
      ],
      "metadata": {
        "id": "ZehmeD8ReAV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a layer to display on a map\n",
        "layer = pdk.Layer(\n",
        "    \"ScatterplotLayer\",\n",
        "    results,\n",
        "    pickable=True,\n",
        "    opacity=0.8,\n",
        "    stroked=True,\n",
        "    filled=True,\n",
        "    radius_scale=20,\n",
        "    radius_min_pixels=1,\n",
        "    radius_max_pixels=100,\n",
        "    line_width_min_pixels=1,\n",
        "    get_position=\"coordinates\",\n",
        "    get_fill_color=[255, 140, 0],\n",
        "    get_line_color=[0, 0, 0],\n",
        ")\n",
        "\n",
        "layer2 = pdk.Layer(\n",
        "    \"ScatterplotLayer\",\n",
        "    gdf,\n",
        "    pickable=True,\n",
        "    opacity=0.8,\n",
        "    stroked=True,\n",
        "    filled=True,\n",
        "    radius_scale=20,\n",
        "    radius_min_pixels=1,\n",
        "    radius_max_pixels=100,\n",
        "    line_width_min_pixels=1,\n",
        "    get_position=\"coordinates\",\n",
        "    get_fill_color=[3, 194, 252],\n",
        "    get_line_color=[0, 0, 0],\n",
        ")\n",
        "\n",
        "# Set the viewport location\n",
        "view_state = pdk.ViewState(latitude=lat, longitude=lon, zoom=zoom_level-1, bearing=0, pitch=45)\n",
        "\n",
        "# Render\n",
        "r = pdk.Deck(layers=[layer2, layer], initial_view_state=view_state, tooltip=tooltip)\n",
        "r.to_html(\"index.html\")"
      ],
      "metadata": {
        "id": "dCRY72uPbVQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search for building apartment"
      ],
      "metadata": {
        "id": "_8oo2om9tFom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def get_place_details(query):\n",
        "    \"\"\"\n",
        "    Search for places using Google Maps search query\n",
        "    Returns a list of all matching places\n",
        "    \"\"\"\n",
        "    base_url = \"https://www.google.com/search\"\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
        "        \"Accept\": \"*/*\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Referer\": \"https://www.google.com/maps\",\n",
        "    }\n",
        "\n",
        "    params = {\n",
        "        \"tbm\": \"map\",\n",
        "        \"authuser\": \"0\",\n",
        "        \"hl\": \"en\",\n",
        "        \"q\": query,\n",
        "        \"pb\": \"!4m12!1m3!1d4313.879391929691!2d91.7362!3d26.1445!\"\n",
        "              \"2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!7i20!8i0!10b1!12m8\"\n",
        "              \"!1m1!18b1!2m3!5m1!6e2!20e3!10b1!16b1!22m1!1e1!29m0!30m1\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            data = response.text\n",
        "\n",
        "            # Find the data section - looking for JSON array\n",
        "            start_idx = data.find(')]}\\'\\n')\n",
        "            if start_idx == -1:\n",
        "                print(\"Could not find start marker in response\")\n",
        "                return None\n",
        "\n",
        "            # Move past the prefix\n",
        "            start_idx += 5\n",
        "\n",
        "            try:\n",
        "                # Parse the JSON data\n",
        "                json_data = json.loads(data[start_idx:])\n",
        "\n",
        "                # Navigate to the search results\n",
        "                if isinstance(json_data, list) and len(json_data) > 0:\n",
        "                    places_data = json_data[0][1]\n",
        "\n",
        "                    results = []\n",
        "\n",
        "                    # Process each place in the results\n",
        "                    for place in places_data[1:]:  # Skip the first element\n",
        "                        try:\n",
        "                            if len(place) > 14:  # Ensure we have enough data\n",
        "                                place_info = {\n",
        "                                    \"name\": place[14][11],\n",
        "                                    \"address\": place[14][2][0] if place[14][2] else None,\n",
        "                                    \"latitude\": place[14][9][2] if place[14][9] else None,\n",
        "                                    \"longitude\": place[14][9][3] if place[14][9] else None,\n",
        "                                    \"place_id\": place[14][78] if len(place[14]) > 78 else None,\n",
        "                                    \"rating\": place[14][4][7] if place[14][4] else None,\n",
        "                                    \"reviews_count\": place[14][4][3][1] if place[14][4] and len(place[14][4]) > 3 else None,\n",
        "                                }\n",
        "\n",
        "                                # Add Google Maps URL if we have a place_id\n",
        "                                if place_info[\"place_id\"]:\n",
        "                                    place_info[\"maps_url\"] = f\"https://www.google.com/maps/place/?q=place_id:{place_info['place_id']}\"\n",
        "\n",
        "                                results.append(place_info)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error processing place: {e}\")\n",
        "                            continue\n",
        "\n",
        "                    return results\n",
        "\n",
        "                return None\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error parsing JSON: {e}\")\n",
        "                print(f\"Response preview: {data[:200]}\")\n",
        "                return None\n",
        "\n",
        "        else:\n",
        "            print(f\"Error: Status code {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "query = \"Abhay Nilima Enclave, Dharapur, Guwahati\"\n",
        "results = get_place_details(query)\n",
        "\n",
        "if results and len(results) > 0:\n",
        "    print(f\"\\nFound {len(results)} places:\")\n",
        "    for idx, place in enumerate(results, 1):\n",
        "        print(f\"\\nResult {idx}:\")\n",
        "        for key, value in place.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(results)\n",
        "    print(\"\\nSaving results to CSV...\")\n",
        "    df.to_csv(\"place_details.csv\", index=False)\n",
        "else:\n",
        "    print(\"No results found\")"
      ],
      "metadata": {
        "id": "lHYwAXRLtI64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_url = \"https://www.google.com/search\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "    \"Accept\": \"*/*\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "    \"Referer\": \"https://www.google.com/\",\n",
        "}\n",
        "\n",
        "params = {\n",
        "    \"tbm\": \"map\",\n",
        "    \"authuser\": \"0\",\n",
        "    \"hl\": \"en\",\n",
        "    \"q\": query,\n",
        "    \"pb\": \"!4m12!1m3!1d4313.879391929691!2d72.82952!3d19.02097!\"\n",
        "    \"2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!7i20!8i0!10b1!12m8\"\n",
        "    \"!1m1!18b1!2m3!5m1!6e2!20e3!10b1!16b1!22m1!1e1!29m0!30m1\",\n",
        "}\n",
        "\n",
        "\n",
        "response = requests.get(base_url, params=params, headers=headers)\n",
        "if response.status_code == 200:\n",
        "    # Extract data from response\n",
        "    data = response.text\n",
        "    # data = data[4:].strip()[:-1]  # Removes \")]}'\" prefix and suffix\n",
        "\n",
        "    data = data[4:].strip()[:-1]\n",
        "    data = data.rstrip(',')  # Remove trailing comma if present\n",
        "    data = data.rstrip(']')  # Remove trailing bracket if present\n",
        "    data = data + ']'        # Add the\n",
        "\n",
        "    json_data = json.loads(data)\n",
        "    # Extract place details from the JSON response\n",
        "    place_data = json_data[0][1]\n",
        "    first_result = place_data[0]\n",
        "    print(first_result)"
      ],
      "metadata": {
        "id": "6yD0_sV8tpOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "lTLxZBbizP7r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}